{
  "name": "jobs post finding",
  "nodes": [
    {
      "parameters": {
        "url": "https://www.linkedin.com/jobs/search/?keywords=data%20engineer&location=Berlin%2C%20Germany&f_TPR=r86400",
        "options": {
          "batching": {
            "batch": {
              "batchSize": 5,
              "batchInterval": 10000
            }
          }
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        592,
        -800
      ],
      "id": "8b5bf13e-8d6b-49a8-a06f-922fa2984c65",
      "name": "linkedin data eng berlin"
    },
    {
      "parameters": {
        "url": "https://www.linkedin.com/jobs/search/?keywords=data%20engineer&location=Copenhagen%2C%20Denmark&f_TPR=r86400",
        "options": {
          "batching": {
            "batch": {
              "batchSize": 5,
              "batchInterval": 10000
            }
          }
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        608,
        -656
      ],
      "id": "000d5013-ddff-4248-b8d2-525116973219",
      "name": "linkedin data eng Copenhagen"
    },
    {
      "parameters": {
        "url": "https://www.linkedin.com/jobs/search/?keywords=data%20engineer&location=Zurich%2C%20Switzerland&f_TPR=r86400",
        "options": {
          "batching": {
            "batch": {
              "batchSize": 5,
              "batchInterval": 10000
            }
          }
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        592,
        -960
      ],
      "id": "269fdcc5-971a-4960-841f-8f5ac1a4e069",
      "name": "linkedin Zurich data eng"
    },
    {
      "parameters": {
        "fieldToSplitOut": "links",
        "options": {}
      },
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [
        1744,
        -944
      ],
      "id": "8afcc7d8-2b05-406d-ab0c-48d49f8e5b8c",
      "name": "Split Out2"
    },
    {
      "parameters": {
        "chatId": "561389208",
        "text": "={{ $json.message }}\n",
        "additionalFields": {
          "parse_mode": "MarkdownV2"
        }
      },
      "type": "n8n-nodes-base.telegram",
      "typeVersion": 1.2,
      "position": [
        3744,
        -592
      ],
      "id": "f744919b-080c-4b82-b694-6492c541e20d",
      "name": "Telegram1",
      "webhookId": "0f29a2c2-c2c0-4424-877a-af37bd97792b",
      "credentials": {
        "telegramApi": {
          "id": "nQqL5wl2KLPfGBZJ",
          "name": "Telegram account"
        }
      }
    },
    {
      "parameters": {
        "url": "={{ $json.links }}",
        "options": {
          "batching": {
            "batch": {
              "batchSize": 5,
              "batchInterval": 10000
            }
          }
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        1456,
        -656
      ],
      "id": "29916eb9-f931-4dc2-b6d0-2976ed5b7536",
      "name": "Scrape Each Job1",
      "retryOnFail": true,
      "executeOnce": false,
      "waitBetweenTries": 5000,
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "import base64, json, re\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse\n\ndef get_html_from_item(item):\n    j = item.get(\"json\") or {}\n    # common html payload fields\n    html = j.get(\"data\") or j.get(\"html\") or j.get(\"body\") or j.get(\"page\") or j.get(\"content\")\n    if html:\n        return html, j\n    # try binary (sometimes HTML comes base64)\n    b = item.get(\"binary\") or {}\n    for _, blob in b.items():\n        data = blob.get(\"data\")\n        if data:\n            try:\n                return base64.b64decode(data).decode(\"utf-8\", \"ignore\"), j\n            except Exception:\n                pass\n    return None, j\n\ndef first_text(soup, selectors):\n    for css in selectors:\n        el = soup.select_one(css)\n        if el:\n            t = el.get_text(\" \", strip=True)\n            if t:\n                return t\n    return \"\"\n\ndef meta_content(soup, names):\n    for n in names:\n        m = soup.find(\"meta\", attrs={\"property\": n}) or soup.find(\"meta\", attrs={\"name\": n})\n        if m and m.get(\"content\"):\n            return m[\"content\"].strip()\n    return \"\"\n\ndef detect_domain(url, soup):\n    host = (urlparse(url).netloc or \"\").lower()\n    if not host:\n        canonical = soup.find(\"link\", rel=\"canonical\")\n        if canonical and canonical.get(\"href\"):\n            host = urlparse(canonical[\"href\"]).netloc.lower()\n    if \"linkedin.\" in host:   return \"linkedin\"\n    if \"boards.greenhouse.io\" in host or \"greenhouse.io\" in host: return \"greenhouse\"\n    if \"lever.co\" in host:    return \"lever\"\n    if \"workable.com\" in host: return \"workable\"\n    if \"indeed.\" in host:     return \"indeed\"\n    return \"generic\"\n\ndef coalesce_location(parts):\n    parts = [p.strip() for p in parts if p and p.strip()]\n    # dedupe while preserving order\n    out, seen = [], set()\n    for p in parts:\n        if p.lower() not in seen:\n            seen.add(p.lower()); out.append(p)\n    return \", \".join(out) if out else \"unknown location\"\n\ndef jsonld_jobposting_locations(soup):\n    \"\"\"Return (title, company, location, url) if found in JSON-LD.\"\"\"\n    for tag in soup.find_all(\"script\", type=lambda v: v and \"ld+json\" in v):\n        try:\n            data = json.loads(tag.string.strip())\n        except Exception:\n            continue\n        # Some pages embed an array of LD objects\n        nodes = data if isinstance(data, list) else [data]\n        for node in nodes:\n            # Look for JobPosting-ish objects\n            typ = node.get(\"@type\") if isinstance(node, dict) else None\n            if isinstance(typ, list):\n                is_job = \"JobPosting\" in typ\n            else:\n                is_job = (typ == \"JobPosting\")\n            if not is_job:\n                continue\n\n            title = node.get(\"title\") or \"\"\n            company = \"\"\n            hiringOrg = node.get(\"hiringOrganization\") or {}\n            if isinstance(hiringOrg, dict):\n                company = hiringOrg.get(\"name\") or \"\"\n            url = node.get(\"hiringOrganization\", {}).get(\"sameAs\") or node.get(\"url\") or \"\"\n\n            # jobLocation can be dict or list\n            locations = []\n            jl = node.get(\"jobLocation\")\n            if jl:\n                jls = jl if isinstance(jl, list) else [jl]\n                for loc in jls:\n                    addr = loc.get(\"address\") if isinstance(loc, dict) else None\n                    if isinstance(addr, dict):\n                        parts = [\n                            addr.get(\"addressLocality\"),\n                            addr.get(\"addressRegion\"),\n                            addr.get(\"addressCountry\"),\n                        ]\n                        locations.append(coalesce_location(parts))\n            # Remote fallback in JSON-LD\n            if not locations:\n                remote_ok = node.get(\"jobLocationType\") or node.get(\"applicantLocationRequirements\")\n                if remote_ok and isinstance(remote_ok, str) and \"Remote\" in remote_ok:\n                    locations.append(\"Remote\")\n\n            location = locations[0] if locations else \"\"\n\n            return (title, company, location, url)\n    return (\"\", \"\", \"\", \"\")\n\nresults = []\n\nfor item in _input.all():\n    html, j = get_html_from_item(item)\n    if not html:\n        continue\n\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    # Prefer the page URL stored by the scraper; fall back to canonical/og:url\n    req_url = (j.get(\"request\", {}) or {}).get(\"url\", \"\") or j.get(\"url\") or \"\"\n    canonical = meta_content(soup, [\"og:url\"]) or (soup.find(\"link\", rel=\"canonical\").get(\"href\") if soup.find(\"link\", rel=\"canonical\") else \"\")\n    page_url = canonical or req_url\n\n    # --- IMPORTANT: do NOT trust any upstream location (prevents \"Switzerland\" carry-over)\n    upstream_location = None\n\n    # JSON-LD first (most reliable across sites)\n    jl_title, jl_company, jl_location, jl_url = jsonld_jobposting_locations(soup)\n\n    domain = detect_domain(page_url, soup)\n\n    # Base fields\n    title = jl_title or meta_content(soup, [\"og:title\", \"twitter:title\"]) or first_text(soup, [\"h1\", \"header h1\"]) or \"unknown title\"\n    company = jl_company or meta_content(soup, [\"og:site_name\"]) or \"\"\n    location = jl_location  # may be \"\"\n\n    # Site-specific fallbacks for title/company/location/description\n    if domain == \"linkedin\":\n        title = title or first_text(soup, [\n            \"h1[class*='top-card-layout__title']\",\n            \"h1[class*='top-card-layout__entity-info'] h1\",\n            \"h1.jobs-unified-top-card__job-title\",\n            \"h1\"\n        ])\n        company = company or first_text(soup, [\n            \"a[class*='topcard__org-name-link']\",\n            \"span[class*='topcard__flavor'] a\",\n            \"span[class*='topcard__flavor']\"\n        ]) or \"unknown company\"\n        if not location:\n            # support multiple LinkedIn locales/DOMs\n            location = first_text(soup, [\n                \"span[class*='topcard__flavor'][class*='--bullet']\",\n                \"span.jobs-unified-top-card__bullet\",\n                \"div.jobs-unified-top-card__primary-description :where(span,li)[class*=bullet]\",\n                \"div.top-card-layout__entity-info span\"\n            ])\n\n        jd = first_text(soup, [\n            \"div.description__text--rich\",\n            \"section.show-more-less-html\",\n            \"div[data-test-description-section]\",\n            \"div.show-more-less-html__markup\"\n        ]) or \"no jd\"\n\n    elif domain == \"greenhouse\":\n        title = title or first_text(soup, [\"h1.app-title\", \"h1.job-title\", \"h1\"])\n        company = company or first_text(soup, [\".header__content .company-name\", \".company-name\"]) or \"unknown company\"\n        if not location:\n            location = first_text(soup, [\".location\", \"div.opening div.location\", \"p.location\"])\n        jd = first_text(soup, [\"div#content\", \"div.content\", \"section#content\", \"div.section-wrapper\"]) or \"no jd\"\n\n    elif domain == \"lever\":\n        title = title or first_text(soup, [\"h2.posting-headline\", \"h2.posting-title\", \"h1\"])\n        company = company or first_text(soup, [\".posting-headline .posting-company\", \".company\"]) or \"unknown company\"\n        if not location:\n            location = first_text(soup, [\".posting-categories .location\", \"div.location\", \"span.location\"])\n        jd = first_text(soup, [\"div.posting\", \"div.description\", \"section.content\"]) or \"no jd\"\n\n    elif domain == \"workable\":\n        title = title or first_text(soup, [\"h1.job-title\", \"h1\", \"header h1\"])\n        company = company or first_text(soup, [\"span.company\", \".job-header .company\"]) or \"unknown company\"\n        if not location:\n            location = first_text(soup, [\"span.job-location\", \".job-location\", \"li.location\", \".location\"])\n        jd = first_text(soup, [\"section.job-description\", \"div#job-description\", \"article\", \"div.description\"]) or \"no jd\"\n\n    elif domain == \"indeed\":\n        title = title or first_text(soup, [\"h1.jobsearch-JobInfoHeader-title\", \"h1\"])\n        company = company or first_text(soup, [\"div.jobsearch-InlineCompanyRating div:nth-child(1)\", \"span.companyName\"]) or \"unknown company\"\n        if not location:\n            location = first_text(soup, [\"div.companyLocation\", \"span.companyLocation\", \"div.jobsearch-InlineCompanyRating div:nth-child(3)\"])\n        jd = first_text(soup, [\"#jobDescriptionText\", \".jobsearch-jobDescriptionText\"]) or \"no jd\"\n\n    else:\n        if not location:\n            location = first_text(soup, [\".location\", \"li.location\", \"span.location\", \"[itemprop='addressLocality']\"]) or \"\"\n        jd = first_text(soup, [\"article\", \"main\", \"section[role='main']\", \"div[role='main']\"]) or \"no jd\"\n\n    # Final location: never use the upstream; prefer JSON-LD > site > generic; clean artifacts\n    location = re.sub(r\"\\s{2,}\", \" \", location).strip() or \"unknown location\"\n\n    results.append({\n        \"title\": title,\n        \"company\": company or \"unknown company\",\n        \"location\": location,\n        \"jd\": jd,\n        \"link\": page_url or (j.get(\"link\") or j.get(\"request\", {}).get(\"url\") or \"\")\n    })\n\nreturn results\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1632,
        -656
      ],
      "id": "243d98dd-18f8-4aad-b4c3-fbc107e9d97d",
      "name": "Parse1",
      "alwaysOutputData": true
    },
    {
      "parameters": {
        "rule": {
          "interval": [
            {}
          ]
        }
      },
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.2,
      "position": [
        -240,
        -944
      ],
      "id": "cd503a80-d5fd-452d-b535-86f477e4ff39",
      "name": "Schedule Trigger1"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "loose",
            "version": 2
          },
          "conditions": [
            {
              "id": "1499f1ac-a7ae-4983-85c7-aa7b8445b2e2",
              "leftValue": "={{ $json.score }}",
              "rightValue": 0.5,
              "operator": {
                "type": "number",
                "operation": "gte"
              }
            }
          ],
          "combinator": "and"
        },
        "looseTypeValidation": true,
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        2944,
        -592
      ],
      "id": "5befa962-88dd-4d81-974a-7e909143273e",
      "name": "Score Filter1"
    },
    {
      "parameters": {
        "jsCode": "/**\n * n8n Function node: build Telegram-safe job messages\n *\n * Expected input formats (per item):\n * 1) item.json.document.pageContent -> will be parsed with extractField()\n * 2) item.json.title, item.json.company, item.json.location, item.json.link -> used directly\n *\n * Keeps the original output format: returns an array of { json: { message: \"...\" } }\n */\n\nfunction escapeMdV2(text) {\n  return String(text || '').replace(/([_*[\\]()~`>#+\\-=|{}.!\\\\])/g, '\\\\$1');\n}\n\nfunction extractField(pageContent, fieldName) {\n  if (!pageContent) return '';\n  if (fieldName === 'link') {\n    const urlMatch = pageContent.match(/https?:\\/\\/[^\\s,}]+/i);\n    if (urlMatch) return urlMatch[0].trim();\n  }\n  const re = new RegExp(fieldName + '\\\\s*:\\\\s*([^,}\\\\n]+)', 'i');\n  const m = pageContent.match(re);\n  if (!m) return '';\n  return m[1].trim().replace(/^[\\s\"'{]+|[\\s\"'}]+$/g, '');\n}\n\n// Split long messages so Telegram can accept them (4096 hard limit)\nfunction splitIntoChunks(text, maxLen = 3500) {\n  const chunks = [];\n  let remaining = text;\n  while (remaining.length > maxLen) {\n    let cut = remaining.lastIndexOf('\\n', maxLen);\n    if (cut === -1) cut = remaining.lastIndexOf(' ', maxLen);\n    if (cut === -1) cut = maxLen;\n    chunks.push(remaining.slice(0, cut).trim());\n    remaining = remaining.slice(cut).trimStart();\n  }\n  if (remaining) chunks.push(remaining);\n  return chunks;\n}\n\n// Build messages using the same \"map\" style you had originally, but allow explicit fields\nconst messages = items.map(item => {\n  // accept explicit fields if provided (preferred), otherwise parse document.pageContent\n  const json = item.json || {};\n  const pc = json.document?.pageContent || '';\n  const title = json.title?.toString().trim() || extractField(pc, 'title');\n  const company = json.company?.toString().trim() || extractField(pc, 'company');\n  const location = json.location?.toString().trim() || extractField(pc, 'location');\n  const link = json.link?.toString().trim() || extractField(pc, 'link');\n  const score = (json.score !== undefined && json.score !== null) ? json.score : '';\n\n  // If both company and location are unknown/empty, skip this item (return empty string -> filtered later)\n  if (\n    (!company && !location) ||\n    (company?.trim().toLowerCase() === 'unknown company' &&\n     location?.trim().toLowerCase() === 'unknown location')\n  ) return '';\n\n\n  return `üë®‚Äçüíº *Title:* ${escapeMdV2(title)}\nüè¢ *Company:* ${escapeMdV2(company)}\nüìç *Location:* ${escapeMdV2(location)}\nüìä *Score:* ${escapeMdV2(score)}\nüîó ${ link ? `[Job Link](${escapeMdV2(link)})` : '_No link provided_' }`;\n});\n\n// Flatten: one output item per (possibly split) message\nconst out = [];\nfor (const msg of messages) {\n  if (!msg) continue; // skipped items become empty strings\n  for (const chunk of splitIntoChunks(msg)) {\n    out.push({ json: { message: chunk } });\n  }\n}\n\nreturn out;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3296,
        -576
      ],
      "id": "735702ac-44c6-4631-8598-4dae08e9d934",
      "name": "single message output1"
    },
    {
      "parameters": {
        "numberInputs": 3
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.2,
      "position": [
        896,
        -912
      ],
      "id": "4f58713c-49ef-4dfa-82bf-aed27bf6c9fb",
      "name": "Merge"
    },
    {
      "parameters": {
        "mode": "insert",
        "memoryKey": {
          "__rl": true,
          "value": "id",
          "mode": "id"
        },
        "embeddingBatchSize": 1000,
        "clearStore": true
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStoreInMemory",
      "typeVersion": 1.3,
      "position": [
        1888,
        -416
      ],
      "id": "a9b1b999-94dd-4943-b62f-8c79003893d8",
      "name": "Simple Vector Store"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "typeVersion": 1.2,
      "position": [
        2000,
        192
      ],
      "id": "662a919e-994e-4c97-9215-b525d0f1e39f",
      "name": "Embeddings OpenAI",
      "credentials": {
        "openAiApi": {
          "id": "i5hndkjFKNMvxU5Q",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "jsonMode": "expressionData",
        "jsonData": "={{ $('Message a model').item.json.message.content }}",
        "textSplittingMode": "custom",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "typeVersion": 1.1,
      "position": [
        2000,
        -256
      ],
      "id": "3e928e40-002a-47d8-821c-006d59689608",
      "name": "Default Data Loader"
    },
    {
      "parameters": {
        "chunkSize": 10000
      },
      "type": "@n8n/n8n-nodes-langchain.textSplitterCharacterTextSplitter",
      "typeVersion": 1,
      "position": [
        2096,
        -128
      ],
      "id": "ff9dd994-101c-4a6e-ad53-f830dbb9ca9e",
      "name": "Character Text Splitter"
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "gpt-4.1-mini",
          "mode": "list",
          "cachedResultName": "GPT-4.1-MINI"
        },
        "messages": {
          "values": [
            {
              "content": "=Read the text in {{ $json.jd }} and EXTRACT ONLY the following fields found verbatim in that text, if not in english translate it first: skills required, years of experience, link to the job, and any mentions that indicate a focus on software engineering and/or data engineering/data science skills. Do NOT add, infer, normalize, paraphrase, expand, or remove items ‚Äî return exactly what appears in the source text.\nIf the text is in another language translate it to ENGLISH.\n\n- For **title**: {{ $json.title }}\n- For **company**: {{ $json.company }}\n- For **location**; {{ $json.location }}\n- For **skills**: list the skills that appear as they are in the job description or ones that are simply deducible by the job description, keep them technical, comma-separated, inside square brackets.\n- For **years_experience**: copy the exact phrase from the text (e.g., \"5+ years\", \"3 years experience\"); if none present, leave blank.\n- For **focus**: copy the exact phrases or short sentences from the text that mention software engineering, data engineering, or data science focus; place them inside square brackets, comma-separated. If none present, leave blank.\n- For **link**: {{ $json.link }}\n\nReturn a single-line string in this exact format (no extra commentary, no line breaks):\n\n{ title: {{ $json.title }}, company: {{ $json.company }}, location: {{ $json.location }}, skills: [ <skill1>, <skill2>, ... ], years_experience: <value>, focus: [ <focus1>, <focus2>, ... ], link: {{$json.link}}}\n",
              "role": "assistant"
            }
          ]
        },
        "options": {
          "temperature": 0
        }
      },
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.8,
      "position": [
        1312,
        -416
      ],
      "id": "cbd50b97-d6d1-42e7-b52c-a83a3243b8c6",
      "name": "Message a model",
      "retryOnFail": true,
      "maxTries": 2,
      "credentials": {
        "openAiApi": {
          "id": "i5hndkjFKNMvxU5Q",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "mode": "load",
        "memoryKey": {
          "__rl": true,
          "value": "id",
          "mode": "list",
          "cachedResultName": "id"
        },
        "prompt": "={{ $json.resume }}",
        "topK": 100
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStoreInMemory",
      "typeVersion": 1.3,
      "position": [
        2672,
        -416
      ],
      "id": "9cfca430-4fe6-4a3f-ac51-f3b798fb3c9f",
      "name": "Simple Vector Store1"
    },
    {
      "parameters": {
        "aggregate": "aggregateAllItemData",
        "options": {}
      },
      "type": "n8n-nodes-base.aggregate",
      "typeVersion": 1,
      "position": [
        2256,
        -512
      ],
      "id": "29f51ebb-b9e9-4cc8-b65a-a0a84a95cdb7",
      "name": "Aggregate"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "54f2e2d0-7062-428b-921f-bb7e5d4ce2ab",
              "name": "resume",
              "value": "title: Data & Machine Learning Engineer, skills: [ Python, SQL, NoSQL, Databricks, Apache Spark, Airflow, Google Cloud Platform, BigQuery, AWS, Azure, Composer, IAM, IBM DataStage, VectorDB, Qdrant, Machine Learning, Artificial Intelligence, Data Engineering, Data Orchestration, ETL, RAG, Scikit-learn, Streamlit, LangChain, Docker, Data Structures & Algorithms, Cloud Deployment, GitHub, N8N ], experience_summary: [ Built scalable data pipelines in Databricks (Medallion Architecture)., Migrated Airflow DAGs to GCP Composer 2.x with 10x efficiency gain., Implemented SCD Type 2 for historical data optimization., Optimized IBM DataStage ETL workflows and SQL queries., Developed RAG pipelines using OpenAI embeddings + vector databases., Researched GANs and AI security risks (deepfakes, membership inference). ], certifications: [ Google Cloud Professional Machine Learning Engineer, AWS Machine Learning Specialty, Databricks Certified Associate Developer for Apache Spark, Databricks Certified Data Engineer Associate, Neo4j Certified Professional ]",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        2496,
        -528
      ],
      "id": "f943b7a0-d27e-42e6-a53c-c04ade1a1b44",
      "name": "INSERT CV"
    },
    {
      "parameters": {
        "content": "## insert CV in the specified format for better job matching\n",
        "height": 192,
        "width": 192
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        2448,
        -752
      ],
      "typeVersion": 1,
      "id": "ce14ced1-dc8c-44bf-ba5a-183a37f93c1f",
      "name": "Sticky Note1"
    },
    {
      "parameters": {
        "content": "## Edit pause if you want it faster, take care of possible scraping blocks from LinkedIn or other sources\n",
        "height": 224
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1120,
        -752
      ],
      "typeVersion": 1,
      "id": "5f3591f1-b423-4759-9fc7-e42cfe62f75e",
      "name": "Sticky Note2"
    },
    {
      "parameters": {
        "content": "## Add whatever LinkedIn search you want, if you want to add other sources, make them work also in the Extract Job Links node\n",
        "height": 272,
        "width": 288
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        512,
        -1312
      ],
      "typeVersion": 1,
      "id": "fff85f12-9a83-4213-a6ca-75250eb35844",
      "name": "Sticky Note3"
    },
    {
      "parameters": {
        "language": "python",
        "pythonCode": "# DROP-IN for \"Extract Job Links\" (multi-site, multi-input, deduped)\n# Supports: linkedin, indeed(.com/ch), jobs.ch, jobscout24.ch, jobup.ch, monster.ch,\n#           swissdevjobs.ch, it-jobs-switzerland.ch, englishjobs.ch, wearedevelopers.com\n# Falls back to a generic heuristic for other career sites.\n\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse, urlunparse, parse_qs\nimport re\n\ndef add_abs(hrefs, base, href):\n    if not href:\n        return\n    hrefs.add(urljoin(base, href))\n\ndef host(url):\n    try:\n        return urlparse(url or \"\").netloc.lower()\n    except Exception:\n        return \"\"\n\ndef detect_domain(domain_hint, start_url):\n    if domain_hint:\n        return domain_hint.lower()\n    h = host(start_url)\n    if not h:\n        return \"generic\"\n    if \"linkedin.\" in h: return \"linkedin\"\n    if \"indeed.\" in h:   return \"indeed\"\n    if \"jobs.ch\" in h:   return \"jobs_ch\"\n    if \"jobscout24.ch\" in h: return \"jobscout24\"\n    if \"jobup.ch\" in h:  return \"jobup\"\n    if \"monster.ch\" in h: return \"monster_ch\"\n    if \"swissdevjobs.ch\" in h: return \"swissdevjobs\"\n    if \"it-jobs-switzerland.ch\" in h: return \"itjobs_switzerland\"\n    if \"englishjobs.ch\" in h: return \"englishjobs\"\n    if \"wearedevelopers.com\" in h: return \"wearedevelopers\"\n    return \"generic\"\n\ndef normalize(u):\n    # remove fragments and trim\n    try:\n        p = urlparse(u.strip())\n        p = p._replace(fragment=\"\")\n        return urlunparse(p)\n    except Exception:\n        return u\n\ndef extract_links(domain_hint, start_url, html):\n    soup = BeautifulSoup(html or \"\", \"html.parser\")\n    hrefs = set()\n    d = detect_domain(domain_hint, start_url)\n\n    # --------- LINKEDIN ----------\n    if d == 'linkedin':\n        # job cards in search results\n        for a in soup.select('ul.jobs-search__results-list li a[class*=\"base-card\"], a[href*=\"/jobs/view/\"]'):\n            add_abs(hrefs, start_url, a.get('href'))\n        normed = []\n        for u in hrefs:\n            p = urlparse(u)\n            # force canonical host and strip tracking on detail pages\n            p = p._replace(netloc='www.linkedin.com')\n            if '/jobs/' in p.path:\n                p = p._replace(query='')\n            normed.append(urlunparse(p))\n        return list(dict.fromkeys(normed))\n\n    # --------- INDEED (.com / ch) ----------\n    if d == 'indeed':\n        # both indeed.com and ch.indeed.com\n        for a in soup.select('a[href*=\"indeed.com/viewjob\"], a[href*=\"/rc/clk\"], a[href*=\"/company/\"]'):\n            add_abs(hrefs, start_url, a.get('href'))\n        cleaned = []\n        for u in hrefs:\n            p = urlparse(u)\n            q = parse_qs(p.query)\n            # keep canonical id param when present\n            if 'jk' in q:\n                p = p._replace(query=f'jk={q[\"jk\"][0]}')\n            cleaned.append(urlunparse(p))\n        return list(dict.fromkeys(cleaned))\n\n    # --------- JOBS.CH ----------\n    if d == 'jobs_ch':\n        # Prefer explicit detail routes (EN / DE / FR)\n        detail_selectors = [\n            'a[href*=\"/en/vacancies/detail/\"]',\n            'a[href*=\"/de/stellenangebote/detail/\"]',\n            'a[href*=\"/fr/offres-emplois/detail/\"]',\n        ]\n        for sel in detail_selectors:\n            for a in soup.select(sel):\n                add_abs(hrefs, start_url, a.get('href'))\n\n        # Fallback: job cards often live in <article> with inner <a> pointing to detail\n        if not hrefs:\n            for a in soup.select('article a[href]'):\n                href = a.get('href') or ''\n                if any(p in href for p in (\"/vacancies/detail/\", \"/stellenangebote/detail/\", \"/offres-emplois/detail/\")):\n                    add_abs(hrefs, start_url, href)\n\n        # Last resort: generic site-wide detail patterns\n        if not hrefs:\n            for a in soup.find_all('a', href=True):\n                h = a['href']\n                if any(s in h for s in (\"/vacancies/detail/\", \"/stellenangebote/detail/\", \"/offres-emplois/detail/\")):\n                    add_abs(hrefs, start_url, h)\n\n        return [normalize(u) for u in hrefs]\n\n\n    # --------- JOBSCOUT24.CH ----------\n    if d == 'jobscout24':\n        # URLs like /en/job/<slug>-<id>/ or /de/job/...\n        for a in soup.select('a[href*=\"/en/job/\"], a[href*=\"/de/job/\"], a[href*=\"/fr/job/\"]'):\n            add_abs(hrefs, start_url, a.get('href'))\n        return [normalize(u) for u in hrefs]\n\n    # --------- JOBUP.CH ----------\n    if d == 'jobup':\n        # Common detail path contains /fr/emplois/ or /en/jobs/ with an id segment\n        for a in soup.select('a[href*=\"/fr/emplois/\"], a[href*=\"/en/jobs/\"], a[href*=\"/de/jobs/\"]'):\n            add_abs(hrefs, start_url, a.get('href'))\n        # fallback: any anchor inside job card containers\n        for a in soup.select('article a[href]'):\n            add_abs(hrefs, start_url, a.get('href'))\n        # keep only job detail-ish paths\n        out = [u for u in hrefs if re.search(r'/(emplois|jobs)/', urlparse(u).path)]\n        return [normalize(u) for u in out]\n\n    # --------- MONSTER.CH ----------\n    if d == 'monster_ch':\n        # Usually /job/ slug\n        for a in soup.select('a[href*=\"/job/\"]'):\n            add_abs(hrefs, start_url, a.get('href'))\n        return [normalize(u) for u in hrefs]\n\n    # --------- SWISSDEVJOBS.CH ----------\n    if d == 'swissdevjobs':\n        # Detail pages: /job/<id or slug>\n        for a in soup.select('a[href*=\"/job/\"]'):\n            add_abs(hrefs, start_url, a.get('href'))\n        return [normalize(u) for u in hrefs]\n\n    # --------- IT-JOBS-SWITZERLAND.CH ----------\n    if d == 'itjobs_switzerland':\n        # Detail pages contain /job/ or /jobs/<id>\n        for a in soup.select('a[href*=\"/job/\"], a[href*=\"/jobs/\"]'):\n            add_abs(hrefs, start_url, a.get('href'))\n        # filter obvious list/pagination links\n        out = [u for u in hrefs if not re.search(r'/search|/page=', u)]\n        return [normalize(u) for u in out]\n\n    # --------- ENGLISHJOBS.CH ----------\n    if d == 'englishjobs':\n        # Many pages use /job/<slug> or /jobs/<slug>\n        for a in soup.select('a[href*=\"/job/\"], a[href*=\"/jobs/\"]'):\n            add_abs(hrefs, start_url, a.get('href'))\n        return [normalize(u) for u in hrefs]\n\n    # --------- WEAREDEVELOPERS.COM ----------\n    if d == 'wearedevelopers':\n        # Detail pages: /jobs/<slug>\n        for a in soup.select('a[href*=\"/jobs/\"]'):\n            href = a.get('href') or ''\n            # skip list filters like /jobs?keyword=\n            if '?' in href:\n                continue\n            add_abs(hrefs, start_url, href)\n        return [normalize(u) for u in hrefs]\n\n    # --------- GENERIC ----------\n    # Generic heuristic: on-site anchors that look like job detail pages\n    for a in soup.find_all('a', href=True):\n        h = a['href']\n        if any(s in h.lower() for s in ['/job/', '/jobs/', '/career', '/careers/', '/position', '/positions/', '/opening', '/vacancy', '/stellen']):\n            add_abs(hrefs, start_url, h)\n    # avoid obvious search/pagination/filter links\n    out = [u for u in hrefs if not re.search(r'(\\?|&)page=|/search|/filters?', u)]\n    return [normalize(u) for u in out]\n\n# ---- process ALL inputs and combine\nall_items = _input.all()\nall_links = []\nseen = set()\n\nfor it in all_items:\n    j = it.get('json', {})\n    domain   = (j.get('domain') or '').strip()\n    startUrl = j.get('startUrl') or j.get('url') or ''\n    html     = j.get('data') or j.get('html') or ''\n\n    links = extract_links(domain, startUrl, html)\n    for u in links:\n        u = normalize(u)\n        if not u:\n            continue\n        # keep only absolute http(s)\n        p = urlparse(u)\n        if p.scheme not in ('http', 'https'):\n            continue\n        if u not in seen:\n            seen.add(u)\n            all_links.append(u)\n\n# Output a SINGLE item with a 'links' array -> your \"Split Out\" node will fan these out\nreturn [{ \"json\": { \"links\": all_links } }]\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1504,
        -944
      ],
      "id": "eaed86ab-502d-4a5c-ac76-e362c6d53e49",
      "name": "Extract Job Links"
    }
  ],
  "pinData": {},
  "connections": {
    "linkedin data eng berlin": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "linkedin data eng Copenhagen": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 2
          }
        ]
      ]
    },
    "linkedin Zurich data eng": {
      "main": [
        [
          {
            "node": "Merge",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Out2": {
      "main": [
        [
          {
            "node": "Scrape Each Job1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Scrape Each Job1": {
      "main": [
        [
          {
            "node": "Parse1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse1": {
      "main": [
        [
          {
            "node": "Message a model",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Schedule Trigger1": {
      "main": [
        [
          {
            "node": "linkedin Zurich data eng",
            "type": "main",
            "index": 0
          },
          {
            "node": "linkedin data eng berlin",
            "type": "main",
            "index": 0
          },
          {
            "node": "linkedin data eng Copenhagen",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Score Filter1": {
      "main": [
        [
          {
            "node": "single message output1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "single message output1": {
      "main": [
        [
          {
            "node": "Telegram1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge": {
      "main": [
        [
          {
            "node": "Extract Job Links",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings OpenAI": {
      "ai_embedding": [
        [
          {
            "node": "Simple Vector Store",
            "type": "ai_embedding",
            "index": 0
          },
          {
            "node": "Simple Vector Store1",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Default Data Loader": {
      "ai_document": [
        [
          {
            "node": "Simple Vector Store",
            "type": "ai_document",
            "index": 0
          }
        ]
      ]
    },
    "Character Text Splitter": {
      "ai_textSplitter": [
        [
          {
            "node": "Default Data Loader",
            "type": "ai_textSplitter",
            "index": 0
          }
        ]
      ]
    },
    "Simple Vector Store": {
      "main": [
        [
          {
            "node": "Aggregate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Message a model": {
      "main": [
        [
          {
            "node": "Simple Vector Store",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Simple Vector Store1": {
      "main": [
        [
          {
            "node": "Score Filter1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Telegram1": {
      "main": [
        []
      ]
    },
    "Aggregate": {
      "main": [
        [
          {
            "node": "INSERT CV",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "INSERT CV": {
      "main": [
        [
          {
            "node": "Simple Vector Store1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract Job Links": {
      "main": [
        [
          {
            "node": "Split Out2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "d8f547ab-5c66-49ec-a1d3-0e465d45c57c",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "190f72b2be14107366b26cc73e0cce4a1de72cdcdfe3d02ca854ee253fe5ed41"
  },
  "id": "UTqfGJ5o7LdRSZTN",
  "tags": []
}